### Update PYSPARK_PYTHON at .profile
export PYSPARK_PYTHON=python3
source .profile

### Update /opt/hive/conf/hive-site.xml with below property.
  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>

### Spark Website to Download. 
https://downloads.apache.org/spark/

### Download Spark
#2.x
wget https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz

#3.x
wget https://ftp.wayne.edu/apache/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
wget https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz 
Important Note:
   3.2.0 was available when I recorded the lecture. But it is not available from 2/6/2022 onwards. Please use 3.2.1 link instead or latest one.
wget https://ftp.wayne.edu/apache/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz


### Untar the File
#2.x
tar xzf spark-2.4.8-bin-hadoop2.7.tgz
#3.x
tar xzf spark-3.2.0-bin-hadoop3.2.tgz
or
tar xzf spark-3.2.1-bin-hadoop3.2.tgz

### Archive the tar file
#2.x
mv spark-2.4.8-bin-hadoop2.7.tgz softwares
#3.x
mv spark-3.2.0-bin-hadoop3.2.tgz softwares
or
mv spark-3.2.1-bin-hadoop3.2.tgz softwares

### Set up Spark Folder Structure
#2.x
sudo mv -f spark-2.4.8-bin-hadoop2.7 /opt
#3.x
sudo mv -f spark-3.2.0-bin-hadoop3.2 /opt
or
sudo mv -f spark-3.2.1-bin-hadoop3.2 /opt

### Set up Soft Link
#2.x
sudo ln -s spark-2.4.8-bin-hadoop2.7 /opt/spark2
#3.x
sudo ln -s spark-3.2.0-bin-hadoop3.2 /opt/spark3
or
sudo ln -s spark-3.2.1-bin-hadoop3.2 /opt/spark3

### spark-env.sh 
#2.x
Update /opt/spark2/conf/spark-env.sh with below environment variables.
export HADOOP_HOME="/opt/hadoop"
export HADOOP_CONF_DIR="/opt/hadoop/etc/hadoop"
export SPARK_DIST_CLASSPATH=$(hadoop --config ${HADOOP_CONF_DIR} classpath)
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native

#3.x
/opt/spark3/conf/spark-env.sh
export HADOOP_HOME="/opt/hadoop"
export HADOOP_CONF_DIR="/opt/hadoop/etc/hadoop"

### spark-defaults.conf
#2.x
Update /opt/spark2/conf/spark-defaults.conf with below properties.
spark.driver.extraJavaOptions -Dderby.system.home=/tmp/derby/
spark.sql.repl.eagerEval.enabled   true
spark.master    yarn
spark.eventLog.enabled true
spark.eventLog.dir               hdfs:///spark2-logs
spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     hdfs:///spark2-logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18081
spark.yarn.historyServer.address localhost:18081
spark.yarn.jars hdfs:///spark2-jars/*.jar

#3.x
Update /opt/spark3/conf/spark-defaults.conf with below properties.
spark.driver.extraJavaOptions     -Dderby.system.home=/tmp/derby/
spark.sql.repl.eagerEval.enabled  true
spark.master                      yarn
spark.eventLog.enabled            true
spark.eventLog.dir                hdfs:///spark3-logs
spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     hdfs:///spark3-logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18080
spark.yarn.historyServer.address  localhost:18080
spark.yarn.jars                   hdfs:///spark3-jars/*.jar

### create directories for logs and jars in HDFS. 
#2.x
hdfs dfs -mkdir /spark2-jars
hdfs dfs -mkdir /spark2-logs
#3.x
hdfs dfs -mkdir /spark3-jars
hdfs dfs -mkdir /spark3-logs


### Copy Spark jars to HDFS folder as part of spark.yarn.jars.
#2.x
hdfs dfs -put /opt/spark2/jars/* /spark2-jars
#3.x
hdfs dfs -put /opt/spark3/jars/* /spark3-jars 

### Integrate Spark with Hive Metastore. Create soft link for hive-site.xml in Spark conf folder.
#2.x
sudo ln -s /opt/hive/conf/hive-site.xml /opt/spark2/conf/
#3.x
sudo ln -s /opt/hive/conf/hive-site.xml /opt/spark3/conf/


### Install  Postgres JDBC jar in Spark jars folder.
#2.x
sudo wget https://jdbc.postgresql.org/download/postgresql-42.2.19.jar \
    -O /opt/spark2/jars/postgresql-42.2.19.jar
#3.x
sudo wget https://jdbc.postgresql.org/download/postgresql-42.2.19.jar \
    -O /opt/spark3/jars/postgresql-42.2.19.jar

### Validate Spark using Scala 
#2.x
/opt/spark2/bin/spark-shell --master yarn --conf spark.ui.port=0
exit by typing :q
#3.x
/opt/spark3/bin/spark-shell --master yarn --conf spark.ui.port=0
exit by typing :q

### Validate Spark using Python 
#2.x
/opt/spark2/bin/pyspark --master yarn --conf spark.ui.port=0
#3.x
/opt/spark3/bin/pyspark3 --master yarn --conf spark.ui.port=0
spark.sql('SHOW databases').show()
spark.sql('USE default')
spark.sql('SELECT * FROM spark').show() 
exit()

### All the commands(spark-shell,pyspark,spark-submit) are avaiable in:
/opt/spark2/bin
/opt/spark3/bin

### Set these commands path at .profile. Add below two lines in .profile
#2.x
export PATH=$PATH:/opt/spark2/bin
#3.x
export PATH=$PATH:/opt/spark3/bin

source .profile

### Distringuish these commands in spark2 and spark3
#2.x
mv /opt/spark2/bin/pyspark /opt/spark2/bin/pyspark2
#3.x
mv /opt/spark3/bin/pyspark /opt/spark3/bin/pyspark3

### Validate the commands
#2.x
pyspark2 --master yarn
#3.x
pyspark3 --master yarn

############ END ###########

